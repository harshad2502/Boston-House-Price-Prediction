{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# Linear regression: Housing Price Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHp3M9ZmrIxj"
      },
      "source": [
        "This notebook uses the classic [Boston Housing](http://lib.stat.cmu.edu/datasets/boston) Dataset and builds a model to predict the housing price in the suburban area of Boston."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras #cleaner and shorter weight\n",
        "from tensorflow.keras import layers #e.g. hidden/ output...\n",
        "from tensorflow.keras.datasets import boston_housing #data\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_72b0LCNbjx"
      },
      "source": [
        "## The Boston Housing Dataset\n",
        "\n",
        "The Boston dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. The dataset is small in size with only 506 cases. A full description of all variables can be found [here](http://lib.stat.cmu.edu/datasets/boston)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFh9ne3FZ-On"
      },
      "source": [
        "### Get the data\n",
        "First download the dataset. Boston dataset is a build-in dataset in Keras. Notice that this build-in dataset has already been cleaned and preprocessed (normalized) by keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9kxxgzvzlyz",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
        "\n",
        "#dataset, which is already separated in train and test data.\n",
        "#The only parameter (path) is basically where to store the downloaded dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "outputs": [],
      "source": [
        "#check training and test size\n",
        "print('Training data : {}'.format(train_data.shape))\n",
        "print('Training target : {}'.format(train_targets.shape))\n",
        "print('Test data : {}'.format(test_data.shape))\n",
        "print('Test target : {}'.format(test_targets.shape))\n",
        "\n",
        "#so can know the regression model: y = g(β0 + β1x1 + ......+ β13x13) : 13 inputs  --> 找β > 1 output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmjdzxKzEu1-"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWtkIjhrZwa"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "Let's build our model. Here, we'll use a `Sequential` model with an output layer that returns a single, continuous value. The model building steps are wrapped in a function, `build_model`, since we'll create a second model, later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiuoYH2FDNIY"
      },
      "outputs": [],
      "source": [
        "# methoud1: directly\n",
        "# Adam\n",
        "model = keras.Sequential([layers.Dense(1, input_shape=[train_data.shape[1]])])\n",
        "#dense: fully connected, CNN就不是dense\n",
        "#1是因為output只有1個\n",
        "optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "#use Adam optimizer here, 0.001 is learning_rate, is default value\n",
        "#https://keras.io/api/optimizers/adam/\n",
        "model.compile(loss='mse',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['mse'])\n",
        "#loss function: tell it is regression problem by MSE, metrics is model evaluation performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlwT-1feDNIZ"
      },
      "outputs": [],
      "source": [
        "#SGD\n",
        "model_1 = keras.Sequential([layers.Dense(1, input_shape=[train_data.shape[1]])])\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(0.0000004)\n",
        "\n",
        "model_1.compile(loss='mse',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['mse'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPKsVjAYDNIa"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "We can also wrap the model in a function, `build_model`, if we want to create a second model, later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c26juK7ZG8j-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# methoud2: writing function for future use\n",
        "# Adam\n",
        "def build_model():\n",
        "    model = keras.Sequential([layers.Dense(1, input_shape=[train_data.shape[1]])])\n",
        "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "    model.compile(loss='mse',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['mse'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj49Og4YGULr"
      },
      "source": [
        "### Inspect the model\n",
        "\n",
        "Use the `.summary` method to print a simple description of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReAD0n6MsFK-"
      },
      "outputs": [],
      "source": [
        "model.summary()\n",
        "#1 output layer\n",
        "#number of parameters: 13 + 1(bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvSFbNphDNIc"
      },
      "outputs": [],
      "source": [
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-qWCsh6DlyH"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Train the model for 2000 epochs, and record the training accuracy in the `history` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD7qHCmNIOY0"
      },
      "outputs": [],
      "source": [
        "# Display training progress by printing a single dot for each completed epoch\n",
        "# Adam\n",
        "class PrintLoss(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        if epoch == 0: self.time = time.monotonic()\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % 100 == 0:\n",
        "            # calculate training time\n",
        "            t = time.monotonic() - self.time\n",
        "            # if validation set is included, its loss is recorded in logs['val_loss']\n",
        "            print('| Epoch {:4} | training time {:6.2f}s | train loss {:6.2f} |'\n",
        "                  .format(epoch, t, logs['loss']))\n",
        "EPOCHS = 3001 #k\n",
        "#only with 2900, so we set = 3001\n",
        "#e.g. sample size = n, total: k*n iterations\n",
        "#try and see when will converge\n",
        "\n",
        "history = model.fit(train_data, train_targets,\n",
        "                    epochs=EPOCHS, verbose=0,\n",
        "                    callbacks=[PrintLoss()])\n",
        "#if batch_size=500，就不會converge (train loss) / if = 404 --> full data set\n",
        "#SGD is very sensitive (when chooseing different stepsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BZqkSlZDNIe"
      },
      "outputs": [],
      "source": [
        "# SDK\n",
        "class PrintLoss(keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        if epoch == 0: self.time = time.monotonic()\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % 100 == 0:\n",
        "            t = time.monotonic() - self.time\n",
        "            print('| Epoch {:4} | training time {:6.2f}s | train loss {:6.2f} |'\n",
        "                  .format(epoch, t, logs['loss']))\n",
        "EPOCHS = 3001 #k\n",
        "\n",
        "history_1 = model_1.fit(train_data, train_targets,\n",
        "                    epochs=EPOCHS, verbose=0,\n",
        "                    callbacks=[PrintLoss()])\n",
        "\n",
        "#SGD is very sensitive (when chooseing different stepsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQm3pc0FYPQB"
      },
      "source": [
        "Visualize the model's training progress using the stats stored in the `history` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xj91b-dymEy"
      },
      "outputs": [],
      "source": [
        "#turn into dataframe\n",
        "hist = pd.DataFrame(history.history)\n",
        "#new add column\n",
        "hist['epoch'] = history.epoch\n",
        "display(hist.tail())\n",
        "\n",
        "#SDK\n",
        "hist_1 = pd.DataFrame(history_1.history)\n",
        "#new add column\n",
        "hist_1['epoch'] = history_1.epoch\n",
        "display(hist_1.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6XriGbVPh2t"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "    hist.tail()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Square Error [MPG]')\n",
        "    plt.plot(hist['epoch'], hist['mse'],\n",
        "             label='Train Error')\n",
        "\n",
        "    plt.show()\n",
        "#Adam\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLEj80R8DNIh"
      },
      "outputs": [],
      "source": [
        "#SDK\n",
        "plot_history(history_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft603OzXuEZC"
      },
      "source": [
        "### Make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MytCgNYXDNIh"
      },
      "source": [
        "Calculate testing test mean squared error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXgtHJpGDNIi"
      },
      "outputs": [],
      "source": [
        "_, mse = model.evaluate(test_data, test_targets, verbose=0)\n",
        "\n",
        "print(\"Adam: Testing set Mean Squared Error: {:5.2f}\".format(mse))\n",
        "\n",
        "#evaluate 印出的值是loss, accuracy(metric)\n",
        "#\"_\"代表最後一次執行的結果，或暫時性/不重要的變數\n",
        "#verbose = 0 代表不要輸出任何紀錄訊息\n",
        "#verbose = 1 表不輸出進度條紀錄，是default，\n",
        "#會出現4/4 [==============================] - 0s 972us/step - loss: 24.4064 - mse: 24.4064\n",
        "\n",
        "_, mse_1 = model_1.evaluate(test_data, test_targets, verbose=0)\n",
        "\n",
        "print(\"SDK: Testing set Mean Squared Error: {:5.2f}\".format(mse_1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18F-TPwuDNIi"
      },
      "source": [
        "Finally, predict housing price using data in the testing set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe7RXH3N3CWU"
      },
      "outputs": [],
      "source": [
        "test_predictions = model.predict(test_data).flatten()\n",
        "\n",
        "plt.scatter(test_targets, test_predictions)\n",
        "plt.xlabel('True Values [price]')\n",
        "plt.ylabel('Predictions [price]')\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.xlim([0,plt.xlim()[1]])\n",
        "plt.ylim([0,plt.ylim()[1]])\n",
        "_ = plt.plot([-100, 100], [-100, 100]) #diagonal line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-rjbqPPDNIj"
      },
      "outputs": [],
      "source": [
        "test_predictions_1 = model_1.predict(test_data).flatten()\n",
        "\n",
        "plt.scatter(test_targets, test_predictions_1)\n",
        "plt.xlabel('True Values [price]')\n",
        "plt.ylabel('Predictions [price]')\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.xlim([0,plt.xlim()[1]])\n",
        "plt.ylim([0,plt.ylim()[1]])\n",
        "_ = plt.plot([-100, 100], [-100, 100]) #diagonal line"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}